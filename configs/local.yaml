# Configuración local para PronunciaPA
version: 1
strict_mode: false  # Modo flexible: usa fallbacks automáticos si algo falla
language_pack: es-mx
# model_pack: model/qwen2.5-7b-instruct  # Comentado: descomentar cuando el modelo esté descargado

# Preprocesamiento
preprocessor:
  name: basic

# ============================================================
# Backend ASR - Modelos con salida IPA directa
# ============================================================
# Opciones disponibles:
#   - allosaurus: Universal, 200+ idiomas, ~500MB, CPU friendly (DEFAULT)
#   - wav2vec2-ipa: Alta precisión, ~1.2GB, requiere GPU para velocidad
#   - xlsr-ipa: Multilingüe 128 idiomas, ~1.2GB, buen balance
#
# Para cambiar de modelo, cambia 'engine' abajo:
# ============================================================
backend:
  name: unified_ipa  # Nuevo backend unificado
  params:
    engine: allosaurus  # Opciones: allosaurus, wav2vec2-ipa, xlsr-ipa
    device: cpu         # cpu, cuda, mps (auto para detectar)
    allosaurus_lang: uni2005  # Modelo Allosaurus recomendado (ipa da error)
    lang: es

# Backend ASR legacy (Allosaurus directo) - ALTERNATIVA
# backend:
#   name: allosaurus
#   params:
#     lang: en

# Backend ASR (Wav2Vec2 ONNX) - DISABLED
# backend:
#   name: onnx
#   params:
#     model_name: wav2vec2-base-960h
#     # Modelo publico de onnx-community (Ingles, pero sirve para validar pipeline)
#     download_url: https://huggingface.co/onnx-community/wav2vec2-base-960h/resolve/main/onnx/model.onnx
#     config_url: https://huggingface.co/onnx-community/wav2vec2-base-960h/resolve/main/config.json
#     vocab_url: https://huggingface.co/onnx-community/wav2vec2-base-960h/resolve/main/vocab.json
#     # Wav2Vec2 specific
#     input_name: input_values
#     output_name: logits

# Conversión texto -> IPA
# eSpeak-NG genera IPA real basado en reglas fonológicas
textref:
  name: espeak
  params:
    default_lang: es

# Comparador
comparator:
  name: levenshtein

# LLM runtime (Ollama - TinyLlama or Phi-3)
llm:
  name: ollama
  params:
    base_url: http://localhost:11434
    model: tinyllama  # or phi3:mini for better quality
    temperature: 0.7
    num_ctx: 4096
    timeout: 120

# Alternative: llama.cpp (uncomment if preferred)
# llm:
#   name: llama_cpp
#   params:
#     binary: llama-cli
#     model_path: data/models/llm/qwen2.5-7b-instruct.gguf
#     n_ctx: 4096
#     n_gpu_layers: 0

# Text-to-speech
# Backend options: espeak (default, no deps), piper (high quality, needs models), system (OS native)
tts:
  name: default
  params:
    prefer: espeak  # Priorizar eSpeak-NG (funciona out-of-the-box)
    system:
      backend: espeak  # Forzar eSpeak-NG explícitamente
      # espeak_bin: "C:\\Program Files\\eSpeak NG\\espeak-ng.exe"  # Descomentar si no está en PATH
      sample_rate: 22050
      channels: 1
    piper:
      # Ajusta estas rutas a tu instalacion de Piper (opcional).
      model_path: data/models/piper/en-us.onnx
      config_path: data/models/piper/en-us.json

# Realtime/WebSocket settings
realtime:
  silence_timeout_ms: 1000  # Esperar 1 segundo de silencio antes de procesar
  chunk_interval_ms: 200    # Intervalo de envío de chunks desde cliente
  max_buffer_seconds: 30    # Máximo buffer antes de forzar procesamiento
  vad:
    energy_threshold: 0.01
    frame_ms: 30

options:
  lang: es
