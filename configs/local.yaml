# Configuración local para PronunciaPA
version: 1
language_pack: en-us
# model_pack: model/qwen2.5-7b-instruct  # Comentado: descomentar cuando el modelo esté descargado

# Preprocesamiento
preprocessor:
  name: basic

# Backend ASR (Allosaurus IPA)
backend:
  name: allosaurus
  params:
    lang: en

# Backend ASR (Wav2Vec2 ONNX) - DISABLED
# backend:
#   name: onnx
#   params:
#     model_name: wav2vec2-base-960h
#     # Modelo publico de onnx-community (Ingles, pero sirve para validar pipeline)
#     download_url: https://huggingface.co/onnx-community/wav2vec2-base-960h/resolve/main/onnx/model.onnx
#     config_url: https://huggingface.co/onnx-community/wav2vec2-base-960h/resolve/main/config.json
#     vocab_url: https://huggingface.co/onnx-community/wav2vec2-base-960h/resolve/main/vocab.json
#     # Wav2Vec2 specific
#     input_name: input_values
#     output_name: logits

# Conversión texto -> IPA (eSpeak/eSpeak-NG)
textref:
  name: espeak
  params:
    default_lang: en

# Comparador
comparator:
  name: levenshtein

# LLM runtime (Ollama - TinyLlama or Phi-3)
llm:
  name: ollama
  params:
    base_url: http://localhost:11434
    model: tinyllama  # or phi3:mini for better quality
    temperature: 0.7
    num_ctx: 4096
    timeout: 120

# Alternative: llama.cpp (uncomment if preferred)
# llm:
#   name: llama_cpp
#   params:
#     binary: llama-cli
#     model_path: data/models/llm/qwen2.5-7b-instruct.gguf
#     n_ctx: 4096
#     n_gpu_layers: 0

# Text-to-speech (Piper con fallback al sistema)
tts:
  name: default
  params:
    prefer: piper
    piper:
      # Ajusta estas rutas a tu instalacion de Piper.
      model_path: data/models/piper/en-us.onnx
      config_path: data/models/piper/en-us.json

options:
  lang: en
